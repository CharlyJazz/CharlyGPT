experiment_name: Experiment 2 - ChatML + Optimizations
model:
  vocab_size: 50257
  context_length: 1024
  emb_dim: 768
  n_heads: 12
  n_layers: 24
  drop_rate: 0.0
  qkv_bias: false
  use_native_gelu: true
  use_native_sdpa: true
  use_weight_tying: true
  use_rope: true
  rope_base: 10000
  use_rmsnorm: false
  use_swiglu: false
data:
  num_samples: 2000000
  max_length: 512
  buffer_size: 10000
  seed: 42
  train_ratio: 0.9
  local_dataset_path: E:\GPT_SANDBOX_STORAGE\SYNTH_DATASET
  use_chatml_format: true
  chat_ml_tokens:
    im_start: <|im_start|>
    im_end: <|im_end|>
    think_start: <think>
    think_end: </think>
training:
  batch_size: 8
  learning_rate: 0.0004
  weight_decay: 0.1
  gradient_clip: 1.0
  warmup_steps: 1000
  total_steps: 398000
  min_learning_rate: 4.0e-05
  dropout_schedule:
    enabled: true
    initial_dropout: 0.0
    final_dropout: 0.05
    transition_at_progress: 0.6
  optimizer:
    type: adamw
    betas:
    - 0.9
    - 0.95
    eps: 1.0e-08
evaluation:
  eval_freq: 1000
  eval_iters: 50
  save_every_n_iterations: 1000
storage:
  base_folder: E:\GPT_SANDBOX_STORAGE
  checkpoint_to_resume: null
hardware:
  device: cuda
runtime:
  model_parameters: 208652544
  trainable_parameters: 208652544
  tokens_per_batch: 4096
