experiment_name: Traditional Small GPT 2 - Samples 2M
data:
  num_samples: 2000000
  max_length: 512
training:
  batch_size: 8
  num_epochs: 2
  learning_rate: 0.0003
  weight_decay: 0.1
  gradient_clip: 1.0
  warmup_steps: 500
evaluation:
  eval_freq: 500
  eval_iters: 20
  save_every_n_iterations: 1000
storage:
  base_folder: E:\GPT_SANDBOX_STORAGE
  checkpoint_to_resume: checkpoint_step_159650.pt
hardware:
  device: cuda
runtime:
  model_parameters: 163009536
  trainable_parameters: 163009536
  tokens_per_batch: 4096
