experiment_name: Traditional Small GPT 2 - Samples 2M v2
data:
  num_samples: 2000000
  max_length: 512
  local_dataset_path: E:\GPT_SANDBOX_STORAGE\SYNTH_DATASET
training:
  batch_size: 16
  learning_rate: 0.0004
  weight_decay: 0.1
  gradient_clip: 1.0
  warmup_steps: 1000
  total_steps: 398000
  min_learning_rate: 4.0e-05
  warmup_ratio: 0.05
  warmdown_ratio: 0.75
evaluation:
  eval_freq: 1000
  eval_iters: 50
  save_every_n_iterations: 1000
storage:
  base_folder: E:\GPT_SANDBOX_STORAGE
  checkpoint_to_resume: checkpoint_step_119936.pt
hardware:
  device: cuda
runtime:
  model_parameters: 163009536
  trainable_parameters: 163009536
  tokens_per_batch: 8192
